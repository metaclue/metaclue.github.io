<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>MetaCLUE</title>
<link href="./MetaCLUE_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./MetaCLUE_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./MetaCLUE_files/jquery.js"></script>
</head>

<body>
<div class="content">
  <h1><strong>MetaCLUE: Towards Systematic Visual Metaphors Research</strong></h1>
  <p id="authors"><span><a href="https://research.google/people/ArjunReddyAkula/"></a></span><a href="https://research.google/people/ArjunReddyAkula/">Arjun R. Akula</a	> <a href="">Brendan Driscoll</a> <a href="">Pradyumna Narayana</a> <a href="">Soravit Changpinyo</a> <br><a href="">Zhiwei Jia,</a> <a href="">Suyash Damle</a> <a href="">Garima Pruthi</a> <a href="">Sugato Basu</a> <br> <a href="">Leonidas Guibas</a><a href="">William T. Freeman</a><a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a> <a href="https://varunjampani.github.io/">Varun Jampani</a> <br>
    <br>
  <span style="font-size: 24px">Google
  </span></p>
  <br>
  <img src="./MetaCLUE_files/teaser_static.jpg" class="teaser-gif" style="width:100%;"><br>
  <h3 style="text-align:center"><em> The first step towards comprehensive evaluation of progress on visual metaphor research…</em></h3>
    <font size="+2">
          <p style="text-align: center;">
            <a href="" target="_blank">[Paper (Coming Soon)]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="" target="_blank">[BibTeX (Coming Soon)]</a>
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Creativity is an indispensable part of human cognition and also an inherent part of how we make sense of the world. Metaphorical abstraction is fundamental in communicating creative ideas through nuanced relationships between abstract concepts such as feelings. While computer vision benchmarks and approaches predominantly focus on understanding and generating literal interpretations of images, metaphorical comprehension of images remains relatively unexplored. Towards this goal, we introduce MetaCLUE, a set of vision tasks on visual metaphor. We also collect high-quality and rich metaphor annotations (abstract objects, concepts, relationships along with their corresponding object boxes) as there do not exist any datasets that facilitate the evaluation of these tasks. We perform a comprehensive analysis of state-of-the-art models in vision and language based on our annotations, highlighting strengths and weaknesses of current approaches in visual metaphor <b>C</b>lassification, <b>L</b>ocalization, <b>U</b>nderstanding (retrieval, question answering, captioning) and g<b>E</b>neration (text-to-image synthesis) tasks. We hope this work provides a concrete step towards systematically developing AI systems with human-like creative capabilities.</p>
</div>
<div class="content">
  <h2>Background</h2>
  <p> Given a particular subject such as clock (shown in the real images on the left), it is very challenging to generate it in different contexts with state-of-the-art text-to-image models, while maintaining high fidelity to its key visual features. Even with dozens of iterations over a text prompt that contains a detailed description of the appearance of the clock (<em>"retro style yellow alarm clock with a white clock face and a yellow number three on the right part of the clock face in the jungle"</em>), the Imagen model [Saharia et al., 2022] can't reconstruct its key visual features (third column). Furthermore, even models whose text embedding lies in a shared language-vision space and can create semantic variations of the image, such as DALL-E2 [Ramesh et al., 2022], can neither reconstruct the appearance of the given subject nor modify the context (second column). In contrast, our approach (right) can synthesize the clock with high fidelity and in new contexts (<em>"a [V] clock in the jungle"</em>).</p>
  <br>
  <img class="summary-img" src="./MetaCLUE_files/background.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Approach</h2>
  <p> Our method takes as input a few images (typically 3-5 images suffice, based on our experiments) of a subject (e.g., a specific dog) and the corresponding class name (e.g. "dog"), and returns a fine-tuned/"personalized'' text-to-image model that encodes a unique identifier that refers to the subject. Then, at inference, we can implant the unique identifier in different sentences to synthesize the subjects in difference contexts.</p>
  <br>
  <img class="summary-img" src="./MetaCLUE_files/high_level.png" style="width:100%;"> <br>
  <p>Given ~3-5 images of a subject we fine tune a text-to-image diffusion in two steps: (a) fine tuning the low-resolution text-to-image model with the input images paired with a text prompt containing a unique identifier and the name of the class the subject belongs to (e.g., "A photo of a [T] dog”), in parallel, we apply a class-specific prior preservation loss, which leverages the semantic prior that the model has on the class and encourages it to generate diverse instances belong to the subject's class by injecting the class name in the text prompt (e.g., "A photo of a dog”). (b) fine-tuning the super resolution components with pairs of low-resolution and high-resolution images taken from our input images set, which enables us to maintain high-fidelity to small details of the subject.</p>
  <br>
  <img class="summary-img" src="./MetaCLUE_files/system.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Results</h2>
  <p>Results for re-contextualization of a bag and vase subject instances. By finetuning a model using our method we are able to generate different images of the a subject instance in different environments, with high preservation of subject details and realistic interaction between the scene and the subject. We display the conditioning prompts below each image. </p>
<img class="summary-img" src="./MetaCLUE_files/results.png" style="width:100%;">
</div>
<div class="content">
  <h2>Art Rendition</h2>
  <p>Original artistic renditions of our subject dog in the style of famous painters. We remark that many of the generated poses were not seen in the training set, such as the Van Gogh and Warhol rendition. We also note that some renditions seem to have novel composition and faithfully imitate the style of the painter - even suggesting some sort of creativity (extrapolation given previous knowledge).</p>
  <br>
  <img class="summary-img" src="./MetaCLUE_files/art.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Text-Guided View Synthesis</h2>
  <p>Our technique can synthesized images with specified viewpoints for a subject cat (left to right: top, bottom, side and back views). Note that the generated poses are  different from the input poses, and the background changes in a realistic manner given a pose change. We also highlight the preservation of complex fur patterns on the subject cat's forehead.</p>
  <br>
  <img class="summary-img" src="./MetaCLUE_files/novel_views.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Property Modification</h2>
  <p>We show color modifications in the first row (using prompts ``a [color] [V] car''), and crosses between a specific dog and different animals in the second row (using prompts ``a cross of a [V] dog and a [target species]''). We highlight the fact that our method preserves unique visual features that give the subject its identity or essence, while performing the required property modification.</p>
  <br>
  <img class="summary-img" src="./MetaCLUE_files/property_modification.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Accessorization</h2>
  <p>Outfitting a dog with accessories. The identity of the subject is preserved and many different outfits or accessories can be applied to the dog given a prompt of type <em>"a [V] dog wearing a police/chef/witch outfit''</em>. We observe a realistic interaction between the subject dog and the outfits or accessories, as well as a large variety of possible options.</p>
  <br>
  <img class="summary-img" src="./MetaCLUE_files/accessories.png" style="width:100%;"> <br>
</div>
<!-- <div class="content">
  <h2>Societal Impact</h2>
  <p>This project aims to provide users with an effective tool for synthesizing personal subjects (animals, objects) in different contexts. While general text-to-image models might be biased towards specific attributes when synthesizing images from text, our approach enables the user to get a better reconstruction of their desirable subjects. On contrary, malicious parties might try to use such images to mislead viewers. This is a common issue, existing in other generative models approaches or content manipulation techniques. Future research in generative modeling, and specifically of personalized generative priors, must continue investigating and revalidating these concerns.</p>
  <br>
</div> -->
<!-- <div class="content">
  <h2>BibTex</h2>
  <code> @article{akula2022metaclue,<br>
  &nbsp;&nbsp;title={MetaCLUE: Towards Systematic Visual Metaphors Research},<br>
  &nbsp;&nbsp;author={Arjun R. Akula, Brendan Driscoll, Pradyumna Narayana, Soravit Changpinyo,Zhiwei Jia, Suyash Damle, Garima Pruthi, Sugato Basu, Leonidas Guibas,William T. Freeman, Yuanzhen Li, Varun Jampan},<br>
  &nbsp;&nbsp;booktitle={},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </code> 
</div> -->
<!-- <div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    We thank Rinon Gal, Adi Zicher, Ron Mokady, Bill Freeman, Dilip Krishnan, Huiwen Chang and Daniel Cohen-Or for their valuable inputs that helped improve this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan for providing us with their support and the pretrained models of Imagen. Finally, a special thank you to David Salesin for his feedback, advice and for his support for the project.
    <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). --> 
  </p>
</div> -->
</body>
</html>
