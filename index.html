<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>MetaCLUE</title>
<link href="./MetaCLUE_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./MetaCLUE_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./MetaCLUE_files/jquery.js"></script>
</head>

<body>
<div class="content">
  <h1><strong>MetaCLUE: Towards Systematic Visual Metaphors Research</strong></h1>
  <p id="authors"><span><a href="https://research.google/people/ArjunReddyAkula/"></a></span><a href="https://research.google/people/ArjunReddyAkula/">Arjun R. Akula</a	> <a href="">Brendan Driscoll</a> <a href="">Pradyumna Narayana</a> <a href="">Soravit Changpinyo</a> <br><a href="">Zhiwei Jia,</a> <a href="">Suyash Damle</a> <a href="">Garima Pruthi</a> <a href="">Sugato Basu</a> <br> <a href="">Leonidas Guibas</a><a href="">William T. Freeman</a><a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a> <a href="https://varunjampani.github.io/">Varun Jampani</a> <br>
    <br>
  <span style="font-size: 24px">Google
  </span></p>
  <br>
  <img src="./MetaCLUE_files/teaser_static.jpg" class="teaser-gif" style="width:100%;"><br>
  <h3 style="text-align:center"><em> The first step towards comprehensive evaluation of progress on visual metaphor researchâ€¦</em></h3>
    <font size="+2">
          <p style="text-align: center;">
            <a href="" target="_blank">[Paper (Coming Soon)]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="" target="_blank">[BibTeX (Coming Soon)]</a>
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Creativity is an indispensable part of human cognition and also an inherent part of how we make sense of the world. Metaphorical abstraction is fundamental in communicating creative ideas through nuanced relationships between abstract concepts such as feelings. While computer vision benchmarks and approaches predominantly focus on understanding and generating literal interpretations of images, metaphorical comprehension of images remains relatively unexplored. Towards this goal, we introduce MetaCLUE, a set of vision tasks on visual metaphor. We also collect high-quality and rich metaphor annotations (abstract objects, concepts, relationships along with their corresponding object boxes) as there do not exist any datasets that facilitate the evaluation of these tasks. We perform a comprehensive analysis of state-of-the-art models in vision and language based on our annotations, highlighting strengths and weaknesses of current approaches in visual metaphor <b>C</b>lassification, <b>L</b>ocalization, <b>U</b>nderstanding (retrieval, question answering, captioning) and g<b>E</b>neration (text-to-image synthesis) tasks. We hope this work provides a concrete step towards systematically developing AI systems with human-like creative capabilities.</p>
</div>
<div class="content">
  <h2>What is a Visual Metaphor?</h2>
  <p> Humans engage metaphors in their creative thinking process as strategies to link or blend concepts, or to view a concept from a target domain in terms of another, apparently dissimilar concept from a source domain [lakoff et al., 2008]. Take as an example <i>'This car is a cheetah'</i>, where <i>This car</i> is compared to <i>'a cheetah'</i> in terms of speed. Metaphors have a simple syntactic structure of <i>'A is B'</i> where A is referred to as the <b>primary concept</b> and B as the <b>secondary concept</b>. The implied analogy in a metaphor is of the form: <i>(primary concept) is as (relationship) as (secondary concept)</i> and often involves an attribute transfer from the secondary to the primary concept. there are at least 4 different types of visual metaphors: <b>Contextual</b>, <b>Hybrid</b>, <b>Juxtaposition</b>, and <b>Multimodal</b> </p>
  <br>
  <img class="summary-img" src="./MetaCLUE_files/background.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Introducing MetaCLUE</h2>
  <p> Much of the computer vision focus on literal interpretation of images. We introduce, MetaCLUE, consisting of four interesting tasks (Classification, Understanding, Localization and Generation) related to metaphorical interpretation and generation of images. </p>
  <br>
  <h2>Results</h2>
  We show models  struggle  to  produce  satisfactory  results  in  many cases, demonstrating the difficulty of these tasks. Below are some examples: <br>
  
  PaLI-17B [Chen et al., 2022], the state-of-the-art literal model for Captioning and VQA performs poorly on MetaCLUE.
  <img class="summary-img" src="./MetaCLUE_files/pali.png" style="width:60%;"> <br> <br>
  
   Sample Localization Results using a CLIP-based phrase-grounding model from [Li et al., 2022], where the secondary concepts are contextual. GT boxes are shown in green, whereas the predictions are shown in blue. We  find  relatively  better  performance  in  localizing secondary objects compared to primary objects.
  <img class="summary-img" src="./MetaCLUE_files/localization.png" style="width:60%;"> <br> <br>
  
  Sample Image Generations for a given metaphorical message (shown on top) with Imagen, Stable Diffusion and fine-tuned (FT) version of Stable Diffusion, indicating the bigscope of improvements in generating visual metaphors.
  <img class="summary-img" src="./MetaCLUE_files/generation.png" style="width:60%;"> <br>
  
  
</div>
  
<!-- <div class="content">
  <h2>Results</h2>
  <p>Results for re-contextualization of a bag and vase subject instances. By finetuning a model using our method we are able to generate different images of the a subject instance in different environments, with high preservation of subject details and realistic interaction between the scene and the subject. We display the conditioning prompts below each image. </p>
<img class="summary-img" src="./MetaCLUE_files/results.png" style="width:100%;">
</div>
<div class="content">
  <h2>Art Rendition</h2>
  <p>Original artistic renditions of our subject dog in the style of famous painters. We remark that many of the generated poses were not seen in the training set, such as the Van Gogh and Warhol rendition. We also note that some renditions seem to have novel composition and faithfully imitate the style of the painter - even suggesting some sort of creativity (extrapolation given previous knowledge).</p>
  <br>
  <img class="summary-img" src="./MetaCLUE_files/art.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Text-Guided View Synthesis</h2>
  <p>Our technique can synthesized images with specified viewpoints for a subject cat (left to right: top, bottom, side and back views). Note that the generated poses are  different from the input poses, and the background changes in a realistic manner given a pose change. We also highlight the preservation of complex fur patterns on the subject cat's forehead.</p>
  <br>
  <img class="summary-img" src="./MetaCLUE_files/novel_views.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Property Modification</h2>
  <p>We show color modifications in the first row (using prompts ``a [color] [V] car''), and crosses between a specific dog and different animals in the second row (using prompts ``a cross of a [V] dog and a [target species]''). We highlight the fact that our method preserves unique visual features that give the subject its identity or essence, while performing the required property modification.</p>
  <br>
  <img class="summary-img" src="./MetaCLUE_files/property_modification.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Accessorization</h2>
  <p>Outfitting a dog with accessories. The identity of the subject is preserved and many different outfits or accessories can be applied to the dog given a prompt of type <em>"a [V] dog wearing a police/chef/witch outfit''</em>. We observe a realistic interaction between the subject dog and the outfits or accessories, as well as a large variety of possible options.</p>
  <br>
  <img class="summary-img" src="./MetaCLUE_files/accessories.png" style="width:100%;"> <br>
</div> -->
<!-- <div class="content">
  <h2>Societal Impact</h2>
  <p>This project aims to provide users with an effective tool for synthesizing personal subjects (animals, objects) in different contexts. While general text-to-image models might be biased towards specific attributes when synthesizing images from text, our approach enables the user to get a better reconstruction of their desirable subjects. On contrary, malicious parties might try to use such images to mislead viewers. This is a common issue, existing in other generative models approaches or content manipulation techniques. Future research in generative modeling, and specifically of personalized generative priors, must continue investigating and revalidating these concerns.</p>
  <br>
</div> -->
<!-- <div class="content">
  <h2>BibTex</h2>
  <code> @article{akula2022metaclue,<br>
  &nbsp;&nbsp;title={MetaCLUE: Towards Systematic Visual Metaphors Research},<br>
  &nbsp;&nbsp;author={Arjun R. Akula, Brendan Driscoll, Pradyumna Narayana, Soravit Changpinyo,Zhiwei Jia, Suyash Damle, Garima Pruthi, Sugato Basu, Leonidas Guibas,William T. Freeman, Yuanzhen Li, Varun Jampan},<br>
  &nbsp;&nbsp;booktitle={},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </code> 
</div> -->
<!-- <div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    We thank Rinon Gal, Adi Zicher, Ron Mokady, Bill Freeman, Dilip Krishnan, Huiwen Chang and Daniel Cohen-Or for their valuable inputs that helped improve this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan for providing us with their support and the pretrained models of Imagen. Finally, a special thank you to David Salesin for his feedback, advice and for his support for the project.
    <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). --> 
  </p>
</div> -->
</body>
</html>
